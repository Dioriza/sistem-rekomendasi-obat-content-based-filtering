# -*- coding: utf-8 -*-
"""Sistem Rekomendasi Obat Content Based SKLearn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XDVmpY-ZG3LqWCRDPJTaofMLeHzUdbbU

**Muhamad Dio Riza Pratama**

**Universitas Buana Perjuangan**

**Dicoding Indonesia - Kampus Merdeka**

# Import Library
"""

pip install emoji

pip install jcopml

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import zipfile
import seaborn as sns
import re

import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

from jcopml.plot import plot_missing_value
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_distances
from sklearn.metrics.pairwise import cosine_similarity

from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
import emoji

"""# Get Dataset

Dataset dipilih melalui platform kaggle, Kaggle adalah komunitas online yang dibangun oleh Goldbloom di tahun 2010. Komunitas ini mengumpulkan para ahli ataupun minat dalam dunia data science yang ingin belajar lebih dalam terkait machine learning ataupun ilmu terkait lainnya. Kaggle telah memiliki lebih dari 1000 dataset, 170.000 post di forum, dan 300 kernel. Oleh karena itu, Kaggle dianggap sebagai platform yang luar biasa hingga diakuisisi Google pada tahun 2017. Kaggle juga platform yang menyediakan open source dataset dan notebook data sains. Berisi komunitas dari penggiat data dari seluruh dunia yang mana berisi aktivitas sharing data, kompetisi data dan pembelajaran data di platform ini.
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!pip install -q kaggle

!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d subhajournal/drug-recommendations

local_zip = '/content/drug-recommendations.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

"""# Import Dataset

Dataset diambil dari Kaggle yang mana berisi data dari Drugs Recomendation yang merupakan data berisi banyak jenis obat, gejala dan ulasan pengguna tentu saja dataset ini sangat berguna untuk rekomendasi obat berdasarkan jenis penyakitnya dan ulasan. Ulasan pengguna akan menjadi faktor yang membantu dalam mengidentifikasi sentimen pengguna berguna untuk meresepkan obat yang sangat cocok untuk penyakit dan gangguan kesehatan tertentu.

Dataset terdiri dari beberapa variabel drugName, Prescribed_for, Drug_Review, User_Rating, Date dan Count_of_Reviews. Karena akan dibuat sistem rekomendasi menggunakan content based filtering, maka ada beberapa variabel yang tidak diperlukan.
"""

df = pd.read_csv('/tmp/Drug_Data.csv')
df = df.drop(columns=['User_Rating', 'Count_of_Reviews', 'Date'])
df.head()

"""# Data Preparation & EDA

Proses awal pada data preparation disini adalah dengan menggabungkan dua variabel yaitu prescribed_for dan Drug_Review. Tujuannya adalah untuk membuat metadata yang membuat model dapat bekerja lebih baik lagi, karena pada dasarnya model yang akan kita buat content based filtering adalah rekomendasi berdasarkan kemiripan. Percobaan pertama telah dibuat dengan hanya menjadikan Drug_review sebagai data yang diencoding tetapi hasilnya kurang memuaskan. Oleh sebab itu disini kita menggabungkan kedua variabel tersebut dan memanfaatkannya untuk memperkembangkan model menjadi variabel baru bernama metadata.
"""

df['metadata'] = df['Prescribed_for'].str.cat(df['Drug_Review'],sep=" ")
df['metadata'] = df['metadata'].apply(str)
df = df.drop(columns=['Drug_Review'])
df.head(10)

"""Selanjutnya adalah pengecekan jumlah baris yang ada pada dataset dicek dengan menggunakan df.shape dan outputnya adalah (53766, 3)

"""

df.shape

"""Tentu banyaknya data ini belum tentu bersih, oleh sebab itu dilakukan pengecekan data yang hilang atau value missing. disini ada dua cara yaitu dengan melihatnya melalui plot dengan bantuan library jcopml ``plot_missing_value(df)`` atau bisa dengan menggunakan bantuan dari library pandas yaitu dengan ``df.isnull().sum().sum())`` dan ternyata data kita ada yang kosong atau nan berjumlah 295 data.

Solusi untuk mengatasi permasalahan ini adalah dengan menghilangkan data yang kita anggap kotor dengan bantuan pandas maka kode untuk menghilangkan data tersebut adalah:

"""

df.info()

plot_missing_value(df)

print("Data NaN berjumlah :",df.isnull().sum().sum())

df = df.dropna() 
print("Data NaN menjadi : ", df.isnull().sum().sum())
df.shape

df

"""untuk melihat penyebaran data top 5 misalnya dari penjelasan mengenai gejala atau penyakit yang diderita(Prescribed_for) dapat dicheck dengan visualisasi data dengan bantuan library matplotlib dan seaborn dengan kode:"""

plt.figure(figsize=(12,10))
sns.set(style="darkgrid")
ax = sns.countplot(x="Prescribed_for", data=df, palette="Set2", order=df['Prescribed_for'].value_counts().index[0:5])

"""Terlihat disini bahwa gejala birth control mendominasi kasus yang ada pada dataset ini dengan penyebaran datanya diatas 9000 kasus. Dilanjutkan dengan Depression, pain, anxiety dan acne. Untuk melihat penyebaran yang lebih lengkap beserta value angka yang ada pada setiap gejala bisa diurutkan dengan kode seperti berikut:"""

Prescribed_for_count=df['Prescribed_for'].value_counts().sort_values(ascending=False)
Prescribed_for_count=pd.DataFrame(Prescribed_for_count)
topPrescribed_for=Prescribed_for_count[0:11]
topPrescribed_for

"""Selanjutnya adalah kita analysis obat apa saja yang paling banyak digunakan oleh pengguna berdasarkan ulasan, dipilih 5 obat teratas dan dengan teknik yang sama seperti diatas dapat dilihat visualisasi dan tabel penyebaran datanya:"""

plt.figure(figsize=(12,10))
sns.set(style="darkgrid")
ax = sns.countplot(x="drugName", data=df, palette="Set2", order=df['drugName'].value_counts().index[0:5])

drug_count=df['drugName'].value_counts().sort_values(ascending=False)
drug_count=pd.DataFrame(drug_count)
topdrug=drug_count[0:11]
topdrug

"""Selanjutnya, banyak isi dari baris data yang kotor berisi angka - angka dan simbol yang tidak diperlukan dan justru mengganggu pada saat proses preprocessing data. maka langka selanjutnya adalah membersihkan data tersebut dari dataset dengan cara menghilangkan simbol dan angka yang tidak diperlukan dengan cara ``df['metadata'] = df['metadata'].str.replace('\d+', '')``"""

#cleaning number
df['metadata'] = df['metadata'].str.replace('\d+', '')
df.head(5)

print('Rata - rata jumlah frasa per baris adalah {0:.0f}.'.format(df.groupby('drugName')['metadata'].count().mean()))

print('Jumlah frasa dalam data: {}. Jumlah kalimat dalam data {}. '.format(df.shape[0], len(df.drugName.unique())))

print('Rata - rata panjang kata dari frasa pada dataset {0:.0f}.'.format(np.mean(df['metadata'].apply(lambda x: len(x.split())))))

have_emoji_train_idx = []
have_emoji_test_idx = []

for idx, review in enumerate(df['metadata']):
    if any(char in emoji.UNICODE_EMOJI for char in 'metadata'):
           have_emoji_train_idx.append(idx)

train_emoji_percentage = round(len(have_emoji_train_idx) / df.metadata.shape[0] * 100, 2)
print(f'Train data has {len(have_emoji_train_idx)} rows that used emoji, that means {train_emoji_percentage} percent of the total')

repeated_rows_train = []

for idx, review in enumerate(df['metadata']):
    if re.match(r'\w*(\w)\1+', review):
        repeated_rows_train.append(idx)

        
print(f'Total {len(repeated_rows_train)} rows')

test1 = "UUUURRGGGEENNTTT"
print(re.sub(r'(\w)\1+', r'\1', test1))

test2 = "good product quality good value for money "
print(re.sub(r'(\w)\1+', r'\1', test2))

def delete_repeated_char(text):
    
    text = re.sub(r'(\w)\1{2,}', r'\1', text)
    
    return text

df.loc[repeated_rows_train, 'metadata'] = df.loc[repeated_rows_train, 'metadata'].apply(delete_repeated_char)

def review_cleaning(text):
    
    # delete lowercase and newline
    text = text.lower()
    text = re.sub(r'\n', '', text)
    
    # change emoticon to text
    text = re.sub(r':\(', 'dislike', text)
    text = re.sub(r': \(\(', 'dislike', text)
    text = re.sub(r':, \(', 'dislike', text)
    text = re.sub(r':\)', 'smile', text)
    text = re.sub(r';\)', 'smile', text)
    text = re.sub(r':\)\)\)', 'smile', text)
    text = re.sub(r':\)\)\)\)\)\)', 'smile', text)
    text = re.sub(r'=\)\)\)\)', 'smile', text)
    
    # delete punctuation
    text = re.sub('[^a-z0-9 ]', ' ', text)
    
    tokenizer = text.split()
    
    return ' '.join([text for text in tokenizer])

df['metadata'] = df['metadata'].apply(review_cleaning)

before = pd.read_csv('/tmp/Drug_Data.csv')

print('Before: ', before.loc[1, 'Drug_Review'])
print('After: ', df.loc[1, 'metadata'])
print('')
print('')
print('Before: ', before.loc[50, 'Drug_Review'])
print('After: ', df.loc[50, 'metadata'])
print('')
print('')
print('Before: ', before.loc[100, 'Drug_Review'])
print('After: ', df.loc[100, 'metadata'])
print('')
print('')
print('Before: ', before.loc[1000, 'Drug_Review'])
print('After: ', df.loc[1000, 'metadata'])

df.head()

"""# Model Selection

## CountVectorizer

Untuk menggunakan data tekstual untuk pemodelan prediktif, teks harus diurai untuk menghapus kata-kata tertentu - proses ini disebut tokenisasi . Kata-kata ini kemudian perlu dienkode sebagai bilangan bulat, atau nilai floating-point, untuk digunakan sebagai masukan dalam algoritme pembelajaran mesin. Proses ini disebut ekstraksi fitur (atau vektorisasi). CountVectorizer digunakan untuk mengonversi kumpulan dokumen teks menjadi vektor jumlah istilah / token. Ini juga memungkinkan pra-pemrosesan data teks sebelum menghasilkan representasi vektor. CountVectorizer dikenal sangat handal dalam memproses tokenisasi pada teks.

CountVectorizer memiliki keunikan dalam mempresentasikan teks ke token dengan value hanya rentang (0,1) dan bisa sangat berguna untuk memproses data dengan alokasi waktu yang sangat cepat, akan tetapi memiliki kelemahan yang sangat besar yaitu bisa mempresentasikan kata yang berbeda tetapi dengan nilai yang serupa. Berikut adalah kelemahan yang dimiliki oleh CountVectorizer:

- Ketidakmampuannya dalam mengidentifikasi kata-kata yang lebih penting dan - kurang penting untuk dianalisis.
- hanya mempertimbangkan kata-kata yang berlimpah di corpus sebagai kata yang paling signifikan secara statistik.
- tidak mengidentifikasi hubungan antar kata seperti kesamaan linguistik antar kata.

Untuk penggunaan CountVectorizer cukup dengan memanggil kode from sklearn.feature_extraction.text import CountVectorizer dan secara default parameter yang diberikan sudah memberikan hasil yang optimal tanpa perlu setting lebih dalam lagi. pada dataset ini parameter yang digunakan hanya stop_words="english", tokenizer = word_tokenize sudah bisa memberikan hasil rekomendasi sistem yang bisa dibilang cukup baik.
"""

bow = CountVectorizer(stop_words="english", tokenizer = word_tokenize)
model = bow.fit_transform(df['metadata'])

"""## TfidfVectorizer

Jika pada CountVectorizer tokenisasi hanya memberikan nilai (0,1) pada setiap kata maka untuk Tfidf value yang diberikan memiliki rentang (0,2) yang mana sesuai dengan perhitungannya setiap kata memiliki representasi tersendiri dan tidak akan memiliki value yang sama seperti dengan CountVectorizer ini adalah salah satu kelebihan yang dimiliki oleh tfidf.

Untuk pemilihan value tinggi atau tidaknya tfidf memiliki cara dengan didasarkan pada logika bahwa kata-kata yang terlalu banyak dalam korpus dan kata-kata yang terlalu jarang keduanya tidak penting secara statistik untuk menemukan pola. Faktor logaritma dalam tfidf secara matematis menghukum kata-kata yang terlalu banyak atau terlalu jarang dalam korpus dengan memberikan nilai tfidf yang rendah.

Nilai tfidf yang lebih tinggi menandakan pentingnya kata-kata yang lebih tinggi dalam korpus sementara nilai yang lebih rendah mewakili kepentingan yang lebih rendah. Dalam contoh gambar di atas kata “AI” hadir dalam kedua kalimat tersebut sedangkan kata “natural” dan “computer” masing-masing hanya ada dalam satu kalimat. Oleh karena itu nilai tfidf “AI” lebih rendah dari dua lainnya. Sedangkan untuk kata “Natural” ada lebih banyak kata di Text1 sehingga kepentingannya lebih rendah daripada “Computer” karena jumlah kata di Text2 lebih sedikit.


Meskipun TFIDF dapat memberikan pemahaman yang baik tentang pentingnya kata-kata tetapi seperti halnya Hitung Vektor, kelemahannya adalah:

- Gagal memberikan informasi linguistik tentang kata-kata seperti arti sebenarnya dari kata-kata itu, kesamaan dengan kata-kata lain, dll.
"""

bow_tfidf = TfidfVectorizer(stop_words="english", tokenizer = word_tokenize)
model_tfidf = bow_tfidf.fit_transform(df['metadata'])

"""## Recommend

## Recommend CountVectorizer
"""

index = 0

dist = cosine_distances(code, model)
content = df.loc[index, "metadata"]
code = bow.transform([content])
rec_index = dist.argsort()[0, 1:10]
print(content)

df.loc[rec_index]

"""## Recommend TfidfVectorizer"""

index = 0

dist_tfidf = cosine_distances(code, model_tfidf)
content = df.loc[index, "metadata"]
code = bow_tfidf.transform([content])
rec_index = dist.argsort()[0, 1:10]
print(content)

df.loc[rec_index]

"""## Evaluate

Metrik yang dipilih adalah cosine distance, cosine similarity dan precision dengan menggunakan bantuan library sklearn. Cosine distance  hanya ditentukan untuk nilai positif nilai dengan rentang terdekat dari 0 adalah yang termirip dengan konten utama dan akan menjadi top-N pada sistem rekomendasi. pada sistem rekomendasi dengan rumus perhitungannya adalah:
![](https://i2.wp.com/hendroprasetyo.com/wp-content/uploads/2020/04/image-9.png?resize=123%2C67&ssl=1)

Untuk cosine simiarity penjelasannya adalah mengukur kesamaan antara dua vektor dan menentukan apakah kedua vektor tersebut menunjuk ke arah yang sama. Ia menghitung sudut cosinus antara dua vektor. Semakin kecil sudut cosinus, semakin besar nilai cosine similarity. Simpelnya adalah semakin besar nilainya ke 1 dari penentuan konten yang dipilih maka akan menjadi top-N pada sistem rekomendasi. Rumus dari cosine similarity adalah:
![](https://dicoding-web-img.sgp1.cdn.digitaloceanspaces.com/original/academy/dos:784efd3d2ba47d47153b050526150ba920210910171725.jpeg)

Precision pada sistem rekomendasi diambil dari top hasil yang relevan kemudian dibagi dengan jumlah item yang direkomendasikan.

![](https://dicoding-web-img.sgp1.cdn.digitaloceanspaces.com/original/academy/dos:819311f78d87da1e0fd8660171fa58e620211012160253.png)

di Python dapat menghitung kedua metrik ini dengan cepat dengan library sklearn dengan kode ``from sklearn.metrics.pairwise import cosine_distances, cosine_similarity``. Pada dataset rekomendasi obat - obatan metrik yang diperoleh dari rekomendasi index ke-0 dipilih top-3 rekomendasi dan akan dilihat kedekatan rekomendasi dari konten dengan cara memilih index top-3 lalu transform ke numpy array, setelah itu dimasukkan ke perhutngan cosine distances dan similarity.

## Evaluate CountVectorizer
"""

content = df.loc[0, "metadata"]
content1 = df.loc[203, "metadata"]
content2 = df.loc[49308	, "metadata"]
content3 = df.loc[7176, "metadata"]
userChoice = bow.transform([content])
rec1 = bow.transform([content1])
rec2 = bow.transform([content2])
rec3 = bow.transform([content3])

dist1 = cosine_distances(userChoice, rec1)
dist2 = cosine_distances(userChoice, rec2)
dist3 = cosine_distances(userChoice, rec3)

print("Cosine distance terdekat yang menjadi rekomendasi dari konten adalah : ", dist1)
print("Cosine distance terdekat yang menjadi rekomendasi dari konten adalah : ", dist2)
print("Cosine distance terdekat yang menjadi rekomendasi dari konten adalah : ", dist3)

dist1 = cosine_similarity(userChoice, rec1)
dist2 = cosine_similarity(userChoice, rec2)
dist3 = cosine_similarity(userChoice, rec3)

print("Kemiripan kata antar baris yang menjadi rekomendasi dari konten adalah : ", dist1)
print("Kemiripan kata antar baris yang menjadi rekomendasi dari konten adalah : ", dist2)
print("Kemiripan kata antar baris yang menjadi rekomendasi dari konten adalah : ", dist3)

"""## Evaluate TfidfVectorizer """

content = df.loc[0, "metadata"]
row1 = df.loc[49308, "metadata"]
row2 = df.loc[17136	, "metadata"]
row3 = df.loc[31915, "metadata"]
userChoice = bow_tfidf.transform([content])
rec1 = bow.transform([row1])
rec2 = bow.transform([row2])
rec3 = bow.transform([row3])

dist1 = cosine_distances(userChoice, rec1)
dist2 = cosine_distances(userChoice, rec2)
dist3 = cosine_distances(userChoice, rec3)

print("Cosine distance terdekat yang menjadi rekomendasi dari konten adalah : ", dist1)
print("Cosine distance terdekat yang menjadi rekomendasi dari konten adalah : ", dist2)
print("Cosine distance terdekat yang menjadi rekomendasi dari konten adalah : ", dist3)

dist1 = cosine_similarity(userChoice, rec1)
dist2 = cosine_similarity(userChoice, rec2)
dist3 = cosine_similarity(userChoice, rec3)

print("Kemiripan kata antar baris yang menjadi rekomendasi dari konten adalah : ", dist1)
print("Kemiripan kata antar baris yang menjadi rekomendasi dari konten adalah : ", dist2)
print("Kemiripan kata antar baris yang menjadi rekomendasi dari konten adalah : ", dist3)

"""## Evaluate Precision """

df.loc[rec_index]

#Precision = of recommendation that are relevant/of item we recommend
#6 diambil dari rekomendasi gejala yang didapat
precision = (6/9) * 100
print("Hasil precision dari rekomendasi sistem adalah ", precision,'%')

#Jika diambil top-5
precision = (4/5) * 100
print("Hasil precision dari rekomendasi sistem adalah ", precision,'%')